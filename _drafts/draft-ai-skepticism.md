---
layout: post
title:  "Uncertainty of AI"
tags: ai society
---

## Background

Growing up, I never really feared AI. One of the first forms I observed in media were advanced robots/droids/androids. C-3PO, Data, Rachel, and Mega Man were all contrasted against their mirrors including the interrogation droid, Borg/Lore, Roy, and Protoman/Sigma, respectively. AI was merely another form of intelligence - not totally superior nor inferior. Something complimentary to organics. Obviously, these examples neglect the complexity of the surrounding environments (or simplicity if you consider post-scarcity in at least one of them), but it never seemed like the future would cause more trouble than it would be worth.

Ray Kurzweil got a lot of attention for the *Singularity*, and at the time, I didn't think much of it. Predictions about the future beyond well understood physics are notoriously bad. Surely, there was nothing to concern ourselves with here. Before that, Deep Blue seemed more like exceptional algorithm and hardware than anything else. Then DeepMind/AlphaGo occurred, and when you compare what [our idea](https://xkcd.com/1002/) of difficulty to the speed at which we have conquered our own limitations, perhaps things were moving closer and closer.

I don't need to even provide context, but generative AI in the post-GPT world has been incredible. For something so relentlessly focused on just textual tokens (yes, I'm aware it's not accurate considering the other transformers relating to sound, music, images, movies, etc.), to do not-trivial things quickly is nothing short of *transformative* achievement.

However, a critical part of AI is in the history of [automation](https://en.wikipedia.org/wiki/Automation). Automation has been in the background the whole time but it's been slow and easier to adapt (although I am sure in retrospect that it was at least moderately disruptive at the time). From the article, I think the truly biggest downsides to automation as it relates to our modern society are the following:

> * Human adaptiveness is often poorly understood by automation initiators. It is often difficult to anticipate every contingency and develop fully preplanned automated responses for every situation. The discoveries inherent in automating processes can require unanticipated iterations to resolve, causing unanticipated costs and delays.
> * People anticipating employment income may be seriously disrupted by others deploying automation where no similar income is readily available.

## The Problem

The crux is that while automation eliminates the need for repetitive tasks while increasing efficiency, we always needed humans to assist in the creation. With the "AI" that we are dealing with now especially agentic versions, it's not yet clear if the negative relating to adaptiveness is true anymore. These models are increasingly capable. Considering the progress made in just the past three years, I'm not sure what the future will hold.

With the advent of "AI", we've seen the following take place:

1. Companies actively seeking to replace the need for people as a product. That is, selling an AI model to act as a person in a role. Could be [secretarial](https://synthflow.ai) [type](https://www.lindy.ai) AI, [AI lawyers](https://ailawyer.pro), and even [AI developers](https://devin.ai).
2. Companies actively seeking to eliminate the need for staff via the use of AI. That is, opting for AI instead of hiring more people. [Shopify CEO did this](https://www.cnbc.com/2025/04/07/shopify-ceo-prove-ai-cant-do-jobs-before-asking-for-more-headcount.html).

Obviously, the success of each of these is highly variable, and at least to me, it's not clear yet what the future holds. But I can say for sure that if humans have to *compete* with AI in areas where humans are , then it will be a slow slide into dystopia.

With historical automation, humans could **not** compete directly. Humans cannot compete against digital control operating at > 1 MHz. But a 1 MHz PLC isn't going to be creating artwork, nor thi